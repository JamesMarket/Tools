import os
import json
import time
import queue
import logging
import tempfile
import threading
import requests
from typing import Dict, List, Optional, Any, Callable
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from seatable_api import Base

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# å¸¸é‡å®šä¹‰
TEMP_DIR = '/ql/scripts/.temp'
STATS_FILE = '/ql/scripts/.stats/seatable_image_sync_stats.json'
IMAGE_BED_URL = 'https://img.shuang.fun/api/tgchannel'
MAX_FILE_SIZE = 5 * 1024 * 1024  # 5MB
REQUEST_DELAY = 1  # è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
MAX_WORKERS = 3  # æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
MAX_QUEUE_SIZE = 1000  # æœ€å¤§é˜Ÿåˆ—å¤§å°

# éœ€è¦å¤„ç†çš„åŸŸååˆ—è¡¨
PROCESS_DOMAINS = [
    'cloud.seatable.cn',
    'cloud.seatable.io'
]

# åˆ›å»ºå¿…è¦çš„ç›®å½•
os.makedirs(TEMP_DIR, exist_ok=True)
os.makedirs(os.path.dirname(STATS_FILE), exist_ok=True)

# å…¨å±€ç»Ÿè®¡æ•°æ®
stats = {
    'bases': 0,
    'tables': 0,
    'images': 0,
    'success': 0,
    'skipped': 0,
    'failed': 0,
    'ignored_domain': 0,
    'from_history': 0,
    'details': {}
}

@dataclass
class ImageTask:
    """å›¾ç‰‡å¤„ç†ä»»åŠ¡"""
    url: str
    table_name: str
    column_name: str
    row_id: str
    base_name: str
    row_data: str = ''
    callback: Optional[Callable] = None

class TaskQueue:
    """ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†"""
    def __init__(self, max_size: int = MAX_QUEUE_SIZE):
        self.queue = queue.Queue(maxsize=max_size)
        self._active = True
        self._lock = threading.Lock()

    def put(self, task: ImageTask):
        """æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—"""
        if self._active:
            self.queue.put(task)

    def get(self) -> Optional[ImageTask]:
        """ä»é˜Ÿåˆ—è·å–ä»»åŠ¡"""
        try:
            return self.queue.get(timeout=1)
        except queue.Empty:
            return None

    def stop(self):
        """åœæ­¢ä»»åŠ¡é˜Ÿåˆ—"""
        with self._lock:
            self._active = False

    @property
    def is_active(self) -> bool:
        """é˜Ÿåˆ—æ˜¯å¦æ´»è·ƒ"""
        with self._lock:
            return self._active

class ImageProcessor:
    """å›¾ç‰‡å¤„ç†å·¥å…·"""
    @staticmethod
    def get_file_extension(url: str) -> str:
        """è·å–æ–‡ä»¶æ‰©å±•å"""
        ext = os.path.splitext(urlparse(url).path)[1]
        return ext.lower() if ext else '.jpg'

    @staticmethod
    def is_valid_image_url(url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å›¾ç‰‡URL"""
        if not url:
            return False
        ext = ImageProcessor.get_file_extension(url)
        valid_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.webp'}
        return ext in valid_extensions

    @staticmethod
    def should_process_domain(url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†è¯¥åŸŸåçš„å›¾ç‰‡"""
        if not url:
            return False
        try:
            domain = urlparse(url).netloc
            return any(process_domain in domain for process_domain in PROCESS_DOMAINS)
        except:
            return False

    @staticmethod
    def format_file_size(size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024:
                return f"{size_bytes:.2f}{unit}"
            size_bytes /= 1024
        return f"{size_bytes:.2f}GB"

    @staticmethod
    def get_temp_file(suffix: str = '.jpg') -> str:
        """è·å–ä¸´æ—¶æ–‡ä»¶è·¯å¾„"""
        temp_file = tempfile.NamedTemporaryFile(
            dir=TEMP_DIR,
            suffix=suffix,
            delete=False
        )
        temp_file.close()
        return temp_file.name

    @staticmethod
    def process_batch(tasks: List[ImageTask], manager: 'SeaTableManager') -> Dict[str, Any]:
        """æ‰¹é‡å¤„ç†å›¾ç‰‡ä»»åŠ¡"""
        results = {}
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_task = {
                executor.submit(manager.process_single_image, task): task
                for task in tasks
            }
            
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                try:
                    result = future.result()
                    results[task.url] = result
                except Exception as e:
                    logger.error(f"å¤„ç†å›¾ç‰‡å¤±è´¥ {task.url}: {str(e)}")
                    results[task.url] = None
                
                # æ·»åŠ è¯·æ±‚å»¶è¿Ÿ
                time.sleep(REQUEST_DELAY)
        
        return results

class ImageHistory:
    """å›¾ç‰‡å¤„ç†å†å²è®°å½•ç®¡ç†"""
    def __init__(self):
        self.current_records = {}
        self._save_lock = threading.Lock()
        self.failed_records = []  # æ–°å¢ï¼šä¸“é—¨å­˜å‚¨å¤±è´¥è®°å½•

    def add_failed_record(self, image_url: str, error_msg: str, base_name: str = '', table_name: str = '', 
                         row_id: str = '', row_data: str = '', column_name: str = ''):
        """æ·»åŠ å¤±è´¥è®°å½•ï¼ŒåŒ…å«å®Œæ•´ä¿¡æ¯"""
        with self._save_lock:
            failed_record = {
                'url': image_url,
                'error': error_msg,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'base_name': base_name,
                'table_name': table_name,
                'row_id': row_id,
                'row_data': row_data,
                'column_name': column_name
            }
            self.failed_records.append(failed_record)
            # åŒæ—¶æ›´æ–° current_records
            self.current_records[image_url] = {
                'status': 'failed',
                'error': error_msg,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'base_name': base_name,
                'table_name': table_name,
                'row_id': row_id,
                'row_data': row_data,
                'column_name': column_name
            }
            logger.info(f"[å†å²] ğŸ“ æ·»åŠ å¤±è´¥è®°å½•: {image_url}")

    def add_success_record(self, image_url: str, image_bed_url: str):
        """æ·»åŠ æˆåŠŸè®°å½•"""
        with self._save_lock:
            # å¦‚æœä¹‹å‰æ˜¯å¤±è´¥è®°å½•ï¼Œä»å¤±è´¥åˆ—è¡¨ä¸­ç§»é™¤
            self.failed_records = [r for r in self.failed_records if r['url'] != image_url]
            self.current_records[image_url] = {
                'status': 'success',
                'image_bed_url': image_bed_url,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }

    def get_record(self, image_url: str) -> Optional[str]:
        """è·å–å†å²è®°å½•"""
        record = self.current_records.get(image_url)
        if record and record.get('status') == 'success':
            return record['image_bed_url']
        return None

    def get_failed_records(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰å¤±è´¥è®°å½•"""
        with self._save_lock:
            failed = self.failed_records.copy()
            logger.info(f"[å†å²] ğŸ“Š å½“å‰å¤±è´¥è®°å½•æ•°: {len(failed)}")
            return failed

    def update_record_status(self, url: str, status: str, image_bed_url: str = None):
        """æ›´æ–°è®°å½•çŠ¶æ€"""
        with self._save_lock:
            if url in self.current_records:
                old_status = self.current_records[url].get('status')
                self.current_records[url]['status'] = status
                if image_bed_url:
                    self.current_records[url]['image_bed_url'] = image_bed_url
                
                # å¦‚æœçŠ¶æ€ä»å¤±è´¥å˜ä¸ºæˆåŠŸï¼Œä»å¤±è´¥åˆ—è¡¨ä¸­ç§»é™¤
                if old_status == 'failed' and status == 'success':
                    self.failed_records = [r for r in self.failed_records if r['url'] != url]
                # å¦‚æœçŠ¶æ€ä»æˆåŠŸå˜ä¸ºå¤±è´¥ï¼Œæ·»åŠ åˆ°å¤±è´¥åˆ—è¡¨
                elif old_status == 'success' and status == 'failed':
                    self.add_failed_record(url, "çŠ¶æ€æ›´æ–°ä¸ºå¤±è´¥")

    def clear_all_records(self):
        """æ¸…ç†æ‰€æœ‰è®°å½•"""
        with self._save_lock:
            failed_count = len(self.failed_records)
            self.current_records.clear()
            self.failed_records.clear()
            logger.info(f"[å†å²] ğŸ§¹ æ¸…ç†æ‰€æœ‰è®°å½•å®Œæˆ (æ¸…ç†äº† {failed_count} æ¡å¤±è´¥è®°å½•)")

class ImageBed:
    """å›¾åºŠç®¡ç†å™¨"""
    def __init__(self, upload_api: str, size_limit: int = 5):
        self.upload_api = upload_api
        self.size_limit = size_limit * 1024 * 1024  # è½¬æ¢ä¸ºå­—èŠ‚
        self.session = create_session()

    def upload_image(self, file_path: str) -> Optional[str]:
        """ä¸Šä¼ å›¾ç‰‡åˆ°å›¾åºŠ"""
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(file_path)
            if file_size > self.size_limit:
                logger.warning(f"[ä¸Šä¼ ] âš ï¸ æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶: {ImageProcessor.format_file_size(file_size)}")
                return None

            # ä¸Šä¼ æ–‡ä»¶
            with open(file_path, 'rb') as f:
                files = {'file': f}
                logger.info(f"[ä¸Šä¼ ] ğŸ“¤ æ­£åœ¨ä¸Šä¼ åˆ°å›¾åºŠ: {self.upload_api}")
                response = self.session.post(self.upload_api, files=files, timeout=60)

            if response.status_code == 200:
                result = response.json()
                if url := result.get('url'):
                    logger.info(f"[ä¸Šä¼ ] âœ… ä¸Šä¼ æˆåŠŸ: {url}")
                    return url
                logger.error(f"[ä¸Šä¼ ] âŒ ä¸Šä¼ å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
            else:
                logger.error(f"[ä¸Šä¼ ] âŒ ä¸Šä¼ å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")

        except Exception as e:
            logger.error(f"[ä¸Šä¼ ] âŒ ä¸Šä¼ è¿‡ç¨‹å‡ºé”™: {str(e)}")

        return None

def create_session():
    """åˆ›å»ºå¸¦é‡è¯•çš„ä¼šè¯"""
    session = requests.Session()
    retry = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET", "POST"]
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

def cleanup_temp_files():
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    try:
        for file in os.listdir(TEMP_DIR):
            file_path = os.path.join(TEMP_DIR, file)
            if os.path.isfile(file_path):
                try:
                    os.unlink(file_path)
                except Exception as e:
                    logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")
    except Exception as e:
        logger.error(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {str(e)}")

def notify_status(title: str, content: str):
    """å‘é€é€šçŸ¥åˆ°é’é¾™é¢æ¿"""
    try:
        url = 'http://localhost:5700/api/sendNotify'
        data = {
            'title': title,
            'content': content
        }
        requests.post(url, json=data, timeout=5)
    except:
        pass  # é€šçŸ¥å¤±è´¥ä¸å½±å“ä¸»æµç¨‹

def update_stats(new_stats: Dict[str, int]):
    """æ›´æ–°è¿è¡Œç»Ÿè®¡"""
    try:
        stats = {}
        if os.path.exists(STATS_FILE):
            with open(STATS_FILE, 'r') as f:
                stats = json.load(f)
                
        stats['last_run'] = time.strftime('%Y-%m-%d %H:%M:%S')
        stats['total_runs'] = stats.get('total_runs', 0) + 1
        stats['total_images'] = stats.get('total_images', 0) + new_stats['images']
        stats['total_success'] = stats.get('total_success', 0) + new_stats['success']
        stats['total_ignored_domain'] = stats.get('total_ignored_domain', 0) + new_stats['ignored_domain']
        
        with open(STATS_FILE, 'w') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
    except:
        pass

def check_environment() -> bool:
    """æ£€æŸ¥è¿è¡Œç¯å¢ƒ"""
    # æ£€æŸ¥å¿…è¦çš„åŒ…
    required_packages = ['seatable_api', 'requests']
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            logger.error(f"ç¼ºå°‘å¿…è¦çš„åŒ…: {package}")
            return False
            
    # æ£€æŸ¥API Tokené…ç½®
    if not os.getenv('SEATABLE_API_TOKENS') and not os.getenv('SEATABLE_API_TOKEN'):
        logger.error("æœªé…ç½®API Token")
        return False
        
    return True

class Config:
    """é…ç½®ç®¡ç†"""
    def __init__(self):
        """åˆå§‹åŒ–é…ç½®"""
        self.config = self._load_from_env()

    def _parse_base_tokens(self, tokens_str: str) -> List[Dict[str, str]]:
        """è§£æbase tokens
        æ ¼å¼1: base_name1:token1,base_name2:token2
        æ ¼å¼2: token1,token2
        """
        if not tokens_str:
            return []
            
        bases = []
        tokens = [t.strip() for t in tokens_str.split(',') if t.strip()]
        
        for token_info in tokens:
            if ':' in token_info:
                # name:token æ ¼å¼
                name, token = token_info.split(':', 1)
                bases.append({'name': name.strip(), 'token': token.strip()})
            else:
                # çº¯tokenæ ¼å¼
                bases.append({'token': token_info})
        
        return bases

    def _load_from_env(self) -> Dict[str, Any]:
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        # è·å–å¹¶è§£æbase tokens
        base_tokens_str = os.getenv('SEATABLE_API_TOKENS') or os.getenv('SEATABLE_API_TOKEN')
        if not base_tokens_str:
            raise Exception("ç¯å¢ƒå˜é‡æœªè®¾ç½®: SEATABLE_API_TOKENS æˆ– SEATABLE_API_TOKEN")
            
        bases = self._parse_base_tokens(base_tokens_str)
        if not bases:
            raise Exception("æ— æ•ˆçš„API Tokené…ç½®")

        return {
            'seatable': {
                'bases': bases,
                'server_url': os.getenv('SEATABLE_SERVER_URL', 'https://cloud.seatable.cn')
            },
            'image_bed': {
                'upload_api': os.getenv('IMAGE_BED_API', 'https://img.shuang.fun/api/tgchannel'),
                'size_limit': int(os.getenv('IMAGE_SIZE_LIMIT', '5'))  # é»˜è®¤5MB
            }
        }

class SeaTableManager:
    """SeaTableç®¡ç†å™¨"""
    def __init__(self, config: Config, api_token: str):
        self.config = config
        self.api_token = api_token
        self.base = self._init_base()
        self._base_name = None
        image_bed_config = config.config['image_bed']
        self.image_bed = ImageBed(
            upload_api=image_bed_config['upload_api'],
            size_limit=image_bed_config['size_limit']
        )
        self.session = create_session()
        self.image_history = ImageHistory()
        self.task_queue = TaskQueue()
        self.processing = False
        self._stats_lock = threading.Lock()
        # æ·»åŠ æ—¥å¿—è®°å½•å­—å…¸
        self.processing_logs = {
            'bases': {},
            'success_records': [],
            'failure_records': [],
            'skip_count': 0,
            'ignored_domain_count': 0
        }

    @property
    def base_name(self) -> str:
        """è·å–baseåç§°çš„å±æ€§"""
        return self._base_name if self._base_name else 'æœªå‘½å'

    @base_name.setter
    def base_name(self, value: str):
        """è®¾ç½®baseåç§°çš„å±æ€§"""
        self._base_name = value

    def _init_base(self) -> Base:
        """åˆå§‹åŒ–SeaTableè¿æ¥"""
        seatable_config = self.config.config['seatable']
        base = Base(self.api_token, seatable_config['server_url'])
        base.auth()
        return base

    def _download_image(self, image_url: str) -> Optional[str]:
        """ä¸‹è½½å›¾ç‰‡"""
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            ext = ImageProcessor.get_file_extension(image_url)
            temp_file = ImageProcessor.get_temp_file(ext)

            logger.info(f"[ä¸‹è½½] ğŸ“¥ å¼€å§‹ä¸‹è½½: {image_url}")

            try:
                self.base.download_file(image_url, temp_file)
                file_size = os.path.getsize(temp_file)
                if file_size > 0:
                    logger.info(f"[ä¸‹è½½] âœ… ä¸‹è½½æˆåŠŸ: {ImageProcessor.format_file_size(file_size)}")
                    return temp_file
                else:
                    logger.error("[ä¸‹è½½] âŒ ä¸‹è½½å¤±è´¥: æ–‡ä»¶å¤§å°ä¸º0")
            except Exception as e:
                logger.error(f"[ä¸‹è½½] âŒ ä¸‹è½½å¤±è´¥: {str(e)}")

            if os.path.exists(temp_file):
                os.unlink(temp_file)
            return None

        except Exception as e:
            logger.error(f"[ä¸‹è½½] âŒ ä¸‹è½½è¿‡ç¨‹å‡ºé”™: {str(e)}")
            return None

    def process_image(self, url: str) -> Optional[str]:
        """å¤„ç†å•ä¸ªå›¾ç‰‡"""
        try:
            # 1. ä¸‹è½½å›¾ç‰‡
            temp_file = self._download_image(url)
            if not temp_file:
                return None

            try:
                # 2. ä¸Šä¼ åˆ°å›¾åºŠ
                new_url = self.image_bed.upload_image(temp_file)
                if new_url:
                    logger.info(f"[å¤„ç†] âœ… æˆåŠŸ: {new_url}")
                return new_url

            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_file):
                    os.unlink(temp_file)

        except Exception as e:
            logger.error(f"[å¤„ç†] âŒ å¤„ç†å¤±è´¥: {str(e)}")
            return None

    def process_single_image(self, task: ImageTask) -> Optional[str]:
        """å¤„ç†å•ä¸ªå›¾ç‰‡ä»»åŠ¡"""
        try:
            # 1. é¦–å…ˆæ£€æŸ¥åŸŸå
            if not ImageProcessor.should_process_domain(task.url):
                with self._stats_lock:
                    stats['ignored_domain'] += 1
                    self._log_ignored_domain(task)
                return None

            # 2. æ£€æŸ¥æ˜¯å¦ä¸ºç©ºURL
            if not task.url:
                with self._stats_lock:
                    stats['skipped'] += 1
                    self._log_skip(task)
                return None

            # 3. æ£€æŸ¥æ˜¯å¦å·²åœ¨å›¾åºŠä¸­
            if 'img.shuang.fun' in task.url:
                with self._stats_lock:
                    stats['skipped'] += 1
                    self._log_skip(task)
                return task.url

            # 4. æ£€æŸ¥å†å²è®°å½•
            if history_url := self.image_history.get_record(task.url):
                with self._stats_lock:
                    stats['from_history'] += 1
                    self._log_success(task, history_url)
                return history_url

            # 5. å¤„ç†æ–°å›¾ç‰‡
            logger.info(f"[å¤„ç†] ğŸ“¥ å¼€å§‹å¤„ç†: {task.url}")
            with self._stats_lock:
                stats['images'] += 1

            # 6. ä¸‹è½½å¹¶ä¸Šä¼ å›¾ç‰‡
            try:
                new_url = self.process_image(task.url)
                
                # 7. æ›´æ–°ç»Ÿè®¡å’Œå†å²è®°å½•
                with self._stats_lock:
                    if new_url:
                        stats['success'] += 1
                        self.image_history.add_success_record(task.url, new_url)
                        self._log_success(task, new_url)
                        logger.info(f"[å¤„ç†] âœ… æˆåŠŸ: {new_url}")
                    else:
                        stats['failed'] += 1
                        error_msg = "ä¸‹è½½æˆ–ä¸Šä¼ å¤±è´¥"
                        self.image_history.add_failed_record(
                            task.url,
                            error_msg,
                            base_name=task.base_name,
                            table_name=task.table_name,
                            row_id=task.row_id,
                            row_data=task.row_data,
                            column_name=task.column_name
                        )
                        self._log_failure(task, error_msg)
                        logger.error(f"[å¤„ç†] âŒ å¤±è´¥: {error_msg}")

                return new_url

            except Exception as e:
                error_msg = str(e)
                with self._stats_lock:
                    stats['failed'] += 1
                    self.image_history.add_failed_record(
                        task.url,
                        error_msg,
                        base_name=task.base_name,
                        table_name=task.table_name,
                        row_id=task.row_id,
                        row_data=task.row_data,
                        column_name=task.column_name
                    )
                    self._log_failure(task, error_msg)
                logger.error(f"[å¤„ç†] âŒ å¤„ç†å‡ºé”™: {error_msg}")
                return None

        except Exception as e:
            error_msg = str(e)
            with self._stats_lock:
                stats['failed'] += 1
                self.image_history.add_failed_record(
                    task.url,
                    error_msg,
                    base_name=task.base_name,
                    table_name=task.table_name,
                    row_id=task.row_id,
                    row_data=task.row_data,
                    column_name=task.column_name
                )
                self._log_failure(task, error_msg)
            logger.error(f"[å¤„ç†] âŒ å¤„ç†å‡ºé”™: {error_msg}")
            return None

    def update_row_callback(self, task: ImageTask, new_url: str):
        """æ›´æ–°è¡Œæ•°æ®çš„å›è°ƒå‡½æ•°"""
        try:
            self.base.update_row(task.table_name, task.row_id, {
                task.column_name: new_url
            })
            logger.info(f"[æ›´æ–°] âœ… {task.table_name} - {task.row_id} - {task.column_name}")
        except Exception as e:
            logger.error(f"[æ›´æ–°] âŒ æ›´æ–°å¤±è´¥: {str(e)}")

    def process_table(self, table_name: str) -> None:
        """å¤„ç†å•ä¸ªè¡¨æ ¼"""
        logger.info(f"\n[è¡¨æ ¼] ğŸ“Š å¼€å§‹å¤„ç†è¡¨æ ¼: {table_name}")
        
        try:
            # è·å–è¡¨æ ¼ä¿¡æ¯
            metadata = self.base.get_metadata()
            table = next((t for t in metadata.get('tables', []) if t['name'] == table_name), None)
            if not table:
                logger.error(f"[è¡¨æ ¼] âŒ è¡¨æ ¼ä¸å­˜åœ¨: {table_name}")
                return

            # è·å–å›¾ç‰‡åˆ—
            image_columns = [col['name'] for col in table.get('columns', []) if col.get('type') == 'image']
            if not image_columns:
                logger.info(f"[è¡¨æ ¼] â„¹ï¸ è¡¨æ ¼ä¸­æ²¡æœ‰å›¾ç‰‡åˆ—ï¼Œè·³è¿‡")
                return

            logger.info(f"[è¡¨æ ¼] ğŸ“· å‘ç°å›¾ç‰‡åˆ—: {', '.join(image_columns)}")

            # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†åˆ—
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = [
                    executor.submit(self.process_column, table_name, column_name)
                    for column_name in image_columns
                    if column_name != "äº§å“å›¾ç‰‡"  # è·³è¿‡äº§å“å›¾ç‰‡åˆ—
                ]
                
                for future in as_completed(futures):
                    try:
                        rows_count = future.result()
                        if rows_count:
                            self._update_table_total_rows(self.base_name, table_name, rows_count)
                    except Exception as e:
                        logger.error(f"[è¡¨æ ¼] âŒ å¤„ç†åˆ—æ—¶å‡ºé”™: {str(e)}")

        except Exception as e:
            logger.error(f"[è¡¨æ ¼] âŒ å¤„ç†è¡¨æ ¼æ—¶å‡ºé”™: {str(e)}")

    def process_column(self, table_name: str, column_name: str) -> Optional[int]:
        """å¤„ç†å•ä¸ªåˆ—"""
        logger.info(f"\n[åˆ—] ğŸ“‘ å¼€å§‹å¤„ç†åˆ—: {column_name}")

        try:
            # åˆ†é¡µå¤„ç†
            page_size = 1000
            start = 0
            total_processed = 0

            while True:
                # è·å–å½“å‰é¡µæ•°æ®
                rows = self.base.list_rows(table_name, start=start, limit=page_size)
                if not rows:
                    break

                if total_processed == 0:
                    logger.info(f"[åˆ—] ğŸ“„ å‘ç° {len(rows)} æ¡è®°å½•")
                total_processed += len(rows)

                # å¤„ç†æ¯ä¸€è¡Œ
                for row in rows:
                    images = row.get(column_name, [])
                    if not images:
                        continue

                    if isinstance(images, str):
                        images = [images]

                    # è·å–é¦–åˆ—å†…å®¹ä½œä¸ºæ ‡è¯†
                    first_column = next(iter(row.keys()))
                    first_column_value = row.get(first_column, '') if first_column != '_id' else ''
                    row_info = f"{first_column_value[:30]}..." if len(str(first_column_value)) > 30 else str(first_column_value)

                    # åˆå§‹åŒ–æ–°å›¾ç‰‡åˆ—è¡¨
                    new_images = []
                    updated = False

                    # å¤„ç†æ¯ä¸ªå›¾ç‰‡
                    for image in images:
                        image_url = image.get('url', '') if isinstance(image, dict) else image
                        
                        task = ImageTask(
                            url=image_url,
                            table_name=table_name,
                            column_name=column_name,
                            row_id=row['_id'],
                            base_name=self.base_name,
                            row_data=row_info
                        )
                        
                        # å¤„ç†å•ä¸ªå›¾ç‰‡
                        new_url = self.process_single_image(task)
                        
                        if new_url:
                            new_images.append(new_url)
                            updated = True
                        else:
                            new_images.append(image)

                    # æ›´æ–°è¡Œæ•°æ®
                    if updated:
                        try:
                            self.base.update_row(table_name, row['_id'], {column_name: new_images})
                            logger.info(f"[æ›´æ–°] âœ… è¡Œæ›´æ–°æˆåŠŸ: {row_info}")
                        except Exception as e:
                            logger.error(f"[æ›´æ–°] âŒ è¡Œæ›´æ–°å¤±è´¥: {str(e)}")

                start += len(rows)
                if len(rows) < page_size:
                    break

                # æ·»åŠ å¤„ç†é—´éš”
                time.sleep(REQUEST_DELAY)

            if total_processed > 0:
                logger.info(f"[åˆ—] âœ¨ å¤„ç†å®Œæˆï¼Œå…± {total_processed} æ¡è®°å½•")
                return total_processed

        except Exception as e:
            logger.error(f"[åˆ—] âŒ å¤„ç†å‡ºé”™: {str(e)}")
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚å¤„ç†

        return None

    def _process_batch_tasks(self, tasks: List[ImageTask]):
        """å¤„ç†ä¸€æ‰¹ä»»åŠ¡"""
        try:
            results = ImageProcessor.process_batch(tasks, self)
            logger.info(f"[æ‰¹å¤„ç†] âœ… å®Œæˆå¤„ç† {len(results)} ä¸ªä»»åŠ¡")
        except Exception as e:
            logger.error(f"[æ‰¹å¤„ç†] âŒ å¤„ç†å¤±è´¥: {str(e)}")

    def retry_failed_images(self):
        """é‡è¯•å¤„ç†å¤±è´¥çš„å›¾ç‰‡"""
        failed_records = self.image_history.get_failed_records()
        if not failed_records:
            logger.info("[é‡è¯•] â„¹ï¸ æ²¡æœ‰å¤±è´¥è®°å½•")
            return

        logger.info(f"\n[é‡è¯•] ğŸ”„ å¼€å§‹å¤„ç† {len(failed_records)} ä¸ªå¤±è´¥è®°å½•")
        
        # åˆå§‹åŒ–é‡è¯•ç»Ÿè®¡
        retry_stats = {
            'total': len(failed_records),
            'success': 0,
            'failed': 0
        }

        # æŒ‰Baseå’Œè¡¨æ ¼åˆ†ç»„å¤„ç†
        grouped_records = self._group_records_by_base_table(failed_records)
        
        # å¤„ç†æ¯ä¸ªåˆ†ç»„
        for base_name, tables in grouped_records.items():
            logger.info(f"\n[é‡è¯•] ğŸ“š å¤„ç†Base: {base_name}")
            for table_name, records in tables.items():
                logger.info(f"[é‡è¯•] ğŸ“‘ å¤„ç†è¡¨æ ¼: {table_name}")
                self._retry_table_records(base_name, table_name, records, retry_stats)

        # è¾“å‡ºé‡è¯•ç»Ÿè®¡
        self._print_retry_stats(retry_stats)

    def _group_records_by_base_table(self, records: List[Dict[str, Any]]) -> Dict[str, Dict[str, List[Dict[str, Any]]]]:
        """å°†å¤±è´¥è®°å½•æŒ‰Baseå’Œè¡¨æ ¼åˆ†ç»„"""
        grouped = {}
        for record in records:
            base_name = record['base_name']
            table_name = record['table_name']
            
            if base_name not in grouped:
                grouped[base_name] = {}
            if table_name not in grouped[base_name]:
                grouped[base_name][table_name] = []
                
            grouped[base_name][table_name].append(record)
        
        return grouped

    def _retry_table_records(self, base_name: str, table_name: str, records: List[Dict[str, Any]], retry_stats: Dict[str, int]):
        """é‡è¯•å¤„ç†åŒä¸€è¡¨æ ¼çš„è®°å½•"""
        # æŒ‰è¡Œåˆ†ç»„ï¼Œé¿å…é‡å¤æ›´æ–°
        rows_to_update = {}
        
        for record in records:
            task = ImageTask(
                url=record['url'],
                table_name=table_name,
                column_name=record['column_name'],
                row_id=record['row_id'],
                base_name=base_name,
                row_data=record['row_data']
            )
            
            # å¤„ç†å•ä¸ªå›¾ç‰‡
            new_url = self.process_image(task.url)  # ç›´æ¥ä½¿ç”¨ process_image è€Œä¸æ˜¯ process_single_image
            
            if new_url:
                retry_stats['success'] += 1
                # è®°å½•éœ€è¦æ›´æ–°çš„è¡Œ
                row_id = record['row_id']
                if row_id not in rows_to_update:
                    rows_to_update[row_id] = {
                        'column': record['column_name'],
                        'urls': []
                    }
                rows_to_update[row_id]['urls'].append(new_url)
                # æ·»åŠ æˆåŠŸè®°å½•
                self.image_history.add_success_record(record['url'], new_url)
            else:
                retry_stats['failed'] += 1
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(REQUEST_DELAY)
        
        # æ‰¹é‡æ›´æ–°è¡Œæ•°æ®
        self._update_rows(table_name, rows_to_update)

    def _update_rows(self, table_name: str, rows_to_update: Dict[str, Dict[str, Any]]):
        """æ‰¹é‡æ›´æ–°è¡Œæ•°æ®"""
        for row_id, data in rows_to_update.items():
            try:
                self.base.update_row(
                    table_name,
                    row_id,
                    {data['column']: data['urls']}
                )
                logger.info(f"[é‡è¯•] âœ… æ›´æ–°æˆåŠŸ: {row_id}")
            except Exception as e:
                logger.error(f"[é‡è¯•] âŒ æ›´æ–°å¤±è´¥ {row_id}: {str(e)}")

    def _print_retry_stats(self, stats: Dict[str, int]):
        """è¾“å‡ºé‡è¯•ç»Ÿè®¡ä¿¡"""
        logger.info("\n" + "=" * 50)
        logger.info("é‡è¯•å¤„ç†æŠ¥å‘Š")
        logger.info("=" * 50)
        logger.info(f"æ€»è®¡é‡è¯•: {stats['total']}")
        logger.info(f"æˆåŠŸè½¬æ¢: {stats['success']}")
        logger.info(f"ä»ç„¶å¤±è´¥: {stats['failed']}")
        logger.info("=" * 50)

    def _log_success(self, task: ImageTask, new_url: str):
        """è®°å½•æˆåŠŸå¤„ç†çš„å›¾ç‰‡"""
        success_record = {
            'base_name': task.base_name,
            'table_name': task.table_name,
            'row_id': task.row_id,
            'column_name': task.column_name,
            'original_url': task.url,
            'new_url': new_url,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        self.processing_logs['success_records'].append(success_record)
        
        # æ›´æ–°baseç»Ÿè®¡
        if task.base_name not in self.processing_logs['bases']:
            self.processing_logs['bases'][task.base_name] = {'tables': {}}
        if task.table_name not in self.processing_logs['bases'][task.base_name]['tables']:
            self.processing_logs['bases'][task.base_name]['tables'][task.table_name] = {
                'total_rows': 0,
                'success_count': 0,
                'failure_count': 0,
                'skip_count': 0,
                'ignored_domain_count': 0,
                'columns': set()
            }
        
        base_stats = self.processing_logs['bases'][task.base_name]['tables'][task.table_name]
        base_stats['success_count'] += 1
        base_stats['columns'].add(task.column_name)

    def _log_failure(self, task: ImageTask, error_msg: str):
        """è®°å½•å¤„ç†å¤±è´¥çš„å›¾ç‰‡"""
        failure_record = {
            'base_name': task.base_name,
            'table_name': task.table_name,
            'row_id': task.row_id,
            'column_name': task.column_name,
            'original_url': task.url,
            'error': error_msg,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        self.processing_logs['failure_records'].append(failure_record)
        
        # æ›´æ–°baseç»Ÿè®¡
        if task.base_name not in self.processing_logs['bases']:
            self.processing_logs['bases'][task.base_name] = {'tables': {}}
        if task.table_name not in self.processing_logs['bases'][task.base_name]['tables']:
            self.processing_logs['bases'][task.base_name]['tables'][task.table_name] = {
                'total_rows': 0,
                'success_count': 0,
                'failure_count': 0,
                'skip_count': 0,
                'ignored_domain_count': 0,
                'columns': set()
            }
        
        base_stats = self.processing_logs['bases'][task.base_name]['tables'][task.table_name]
        base_stats['failure_count'] += 1
        base_stats['columns'].add(task.column_name)

    def _log_skip(self, task: ImageTask):
        """è®°å½•è·³è¿‡çš„å›¾ç‰‡"""
        self.processing_logs['skip_count'] += 1
        
        # æ›´æ–°baseç»Ÿè®¡
        if task.base_name not in self.processing_logs['bases']:
            self.processing_logs['bases'][task.base_name] = {'tables': {}}
        if task.table_name not in self.processing_logs['bases'][task.base_name]['tables']:
            self.processing_logs['bases'][task.base_name]['tables'][task.table_name] = {
                'total_rows': 0,
                'success_count': 0,
                'failure_count': 0,
                'skip_count': 0,
                'ignored_domain_count': 0,
                'columns': set()
            }
        
        base_stats = self.processing_logs['bases'][task.base_name]['tables'][task.table_name]
        base_stats['skip_count'] += 1

    def _log_ignored_domain(self, task: ImageTask):
        """è®°å½•ä¸å¤„ç†åŸŸåçš„å›¾ç‰‡"""
        self.processing_logs['ignored_domain_count'] += 1
        
        # æ›´æ–°baseç»Ÿè®¡
        if task.base_name not in self.processing_logs['bases']:
            self.processing_logs['bases'][task.base_name] = {'tables': {}}
        if task.table_name not in self.processing_logs['bases'][task.base_name]['tables']:
            self.processing_logs['bases'][task.base_name]['tables'][task.table_name] = {
                'total_rows': 0,
                'success_count': 0,
                'failure_count': 0,
                'skip_count': 0,
                'ignored_domain_count': 0,
                'columns': set()
            }
        
        base_stats = self.processing_logs['bases'][task.base_name]['tables'][task.table_name]
        base_stats['ignored_domain_count'] += 1

    def _update_table_total_rows(self, base_name: str, table_name: str, total_rows: int):
        """æ›´æ–°è¡¨æ ¼æ€»è¡Œæ•°"""
        if base_name not in self.processing_logs['bases']:
            self.processing_logs['bases'][base_name] = {'tables': {}}
        if table_name not in self.processing_logs['bases'][base_name]['tables']:
            self.processing_logs['bases'][base_name]['tables'][table_name] = {
                'total_rows': 0,
                'success_count': 0,
                'failure_count': 0,
                'skip_count': 0,
                'ignored_domain_count': 0,
                'columns': set()
            }
        
        self.processing_logs['bases'][base_name]['tables'][table_name]['total_rows'] = total_rows

def generate_report(manager: SeaTableManager, duration: float) -> str:
    """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
    lines = [
        "\n" + "=" * 50,
        f"å¤„ç†æŠ¥å‘Š - {time.strftime('%Y-%m-%d %H:%M:%S')}",
        "=" * 50,
        ""
    ]

    # å¤„ç†æ¯ä¸ªbaseçš„è¯¦ç»†ä¿¡æ¯
    for base_name, base_info in manager.processing_logs['bases'].items():
        lines.extend([
            f"[Base: {base_name}]",
            "--------------------"
        ])

        # å¤„ç†æ¯ä¸ªè¡¨æ ¼
        for table_name, table_stats in base_info['tables'].items():
            lines.extend([
                f"è¡¨æ ¼: {table_name}",
                f"  - æ€»è¡Œæ•°: {table_stats['total_rows']}",
                f"  - å¤„ç†åˆ—: {', '.join(sorted(table_stats['columns']))}"
            ])

            # æ·»åŠ æˆåŠŸè®°å½•
            success_records = [r for r in manager.processing_logs['success_records'] 
                             if r['base_name'] == base_name and r['table_name'] == table_name]
            if success_records:
                lines.append("\n  æˆåŠŸè®°å½•:")
                for i, record in enumerate(success_records, 1):
                    lines.extend([
                        f"  {i}. è¡ŒID: {record['row_id']}",
                        f"     - åˆ—: {record['column_name']}",
                        f"     - åŸå›¾: {record['original_url']}",
                        f"     - æ–°å›¾: {record['new_url']}"
                    ])

            # æ·»åŠ å¤±è´¥è®°å½•
            failure_records = [r for r in manager.processing_logs['failure_records']
                             if r['base_name'] == base_name and r['table_name'] == table_name]
            if failure_records:
                lines.append("\n  å¤±è´¥è®°å½•:")
                for i, record in enumerate(failure_records, 1):
                    lines.extend([
                        f"  {i}. è¡ŒID: {record['row_id']}",
                        f"     - åˆ—: {record['column_name']}",
                        f"     - åŸå›¾: {record['original_url']}",
                        f"     - é”™è¯¯: {record['error']}",
                        f"     - æ—¶é—´: {record['timestamp']}"
                    ])

            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            lines.extend([
                "\n  ç»Ÿè®¡ä¿¡æ¯:",
                f"  - è·³è¿‡å›¾ç‰‡: {table_stats['skip_count']}æ¡",
                f"  - ä¸å¤„ç†åŸŸå: {table_stats['ignored_domain_count']}æ¡",
                f"  - æˆåŠŸè½¬å­˜: {table_stats['success_count']}æ¡",
                f"  - å¤±è´¥å›¾ç‰‡: {table_stats['failure_count']}æ¡",
                ""
            ])

    # æ·»åŠ æ€»ä½“ç»Ÿè®¡
    lines.extend([
        "=" * 50,
        "æ€»ä½“ç»Ÿè®¡",
        "=" * 50,
        f"- å¤„ç†Baseæ•°: {len(manager.processing_logs['bases'])}",
        f"- å¤„ç†è¡¨æ ¼æ•°: {sum(len(base_info['tables']) for base_info in manager.processing_logs['bases'].values())}",
        f"- å¤„ç†å›¾ç‰‡æ•°: {len(manager.processing_logs['success_records']) + len(manager.processing_logs['failure_records'])}",
        f"- æˆåŠŸè½¬å­˜: {len(manager.processing_logs['success_records'])}",
        f"- å¤±è´¥å›¾ç‰‡: {len(manager.processing_logs['failure_records'])}",
        f"- è·³è¿‡å›¾ç‰‡: {manager.processing_logs['skip_count']}",
        f"- ä¸å¤„ç†åŸŸå: {manager.processing_logs['ignored_domain_count']}",
        f"- æ‰§è¡Œæ—¶é—´: {duration:.2f}ç§’",
        ""
    ])

    # æ·»åŠ å¤±è´¥è®°å½•æ±‡æ€»
    if manager.processing_logs['failure_records']:
        lines.extend([
            "=" * 50,
            "å¤±è´¥è®°å½•æ±‡æ€»",
            "=" * 50
        ])
        
        current_base = None
        for record in manager.processing_logs['failure_records']:
            if current_base != record['base_name']:
                current_base = record['base_name']
                lines.append(f"\nBase: {current_base}")
            
            lines.extend([
                f"- {record['row_id']} ({record['table_name']})",
                f"  - åˆ—: {record['column_name']}",
                f"  - åŸå›¾: {record['original_url']}",
                f"  - é”™è¯¯: {record['error']}",
                f"  - æ—¶é—´: {record['timestamp']}"
            ])

    return "\n".join(lines)

def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    global stats
    stats = {
        'bases': 0,
        'tables': 0,
        'images': 0,
        'success': 0,
        'skipped': 0,
        'failed': 0,
        'ignored_domain': 0,
        'from_history': 0,
        'details': {}
    }

    try:
        # æ¸…ç†æ—§çš„ä¸´æ—¶æ–‡ä»¶
        cleanup_temp_files()
        
        # æ£€æŸ¥ç¯å¢ƒ
        if not check_environment():
            return
        
        # åŠ è½½é…ç½®
        config = Config()
        
        # è·å–æ‰€æœ‰baseé…ç½®
        bases = config.config['seatable']['bases']
        logger.info(f"[ä¸»ç¨‹åº] ğŸ“š å‘ç° {len(bases)} ä¸ªbaseå¾…å¤„ç†")
        
        # ç”¨äºè·Ÿè¸ªæœªå‘½åbaseçš„è®¡æ•°
        unnamed_count = 0
        base_names = set()
        
        # åˆ›å»ºä¸€ä¸ªå…¨å±€çš„å›¾ç‰‡å†å²è®°å½•ç®¡ç†å™¨
        image_history = ImageHistory()
        
        # å¤„ç†æ¯ä¸ªbase
        for base_config in bases:
            base_token = base_config.get('token')
            config_base_name = base_config.get('name')
            
            try:
                # åˆå§‹åŒ–SeaTableç®¡ç†å™¨
                manager = SeaTableManager(config, base_token)
                manager.image_history = image_history  # ä½¿ç”¨å…¨å±€çš„å†å²è®°å½•ç®¡ç†å™¨
                
                # è·å–baseå…ƒæ•°æ®
                metadata = manager.base.get_metadata()
                
                # ç¡®å®šbaseåç§°
                if config_base_name:
                    # ä½¿ç”¨é…ç½®ä¸­çš„åç§°
                    base_name = config_base_name
                else:
                    # ä½¿ç”¨APIè¿”å›çš„åç§°ï¼Œå¦‚æœé‡å¤åˆ™æ·»åŠ åºå·
                    base_name = metadata.get('name', 'æœªå‘½å')
                    if base_name == 'æœªå‘½å' or base_name in base_names:
                        unnamed_count += 1
                        base_name = f'æœªå‘½å{unnamed_count}'
                
                manager.base_name = base_name  # æ›´æ–°managerä¸­çš„baseåç§°
                base_names.add(base_name)  # è®°å½•ä½¿ç”¨è¿‡çš„åç§°
                
                logger.info(f"\n[Base] ğŸ”„ å¼€å§‹å¤„ç†base: {base_name}")
                
                # è·å–æ‰€æœ‰è¡¨æ ¼
                tables = metadata.get('tables', [])
                
                if not tables:
                    logger.error(f"[Base] âŒ {base_name} æœªæ‰¾åˆ°ä»»ä½•è¡¨æ ¼")
                    continue
                
                logger.info(f"[Base] å‘ç° {base_name} æœ‰ {len(tables)} ä¸ªè¡¨æ ¼")
                stats['bases'] += 1
                stats['details'][base_name] = {'tables': {}}
                
                # å¤„ç†æ¯ä¸ªè¡¨æ ¼
                for table in tables:
                    table_name = table['name']
                    stats['details'][base_name]['tables'][table_name] = {'columns': {}}
                    manager.process_table(table_name)
                    
                logger.info(f"[Base] âœ¨ {base_name} å¤„ç†å®Œæˆ")
                
            except Exception as e:
                logger.error(f"[Base] âŒ {base_name} å¤„ç†å‡ºé”™: {str(e)}")
                continue
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        update_stats(stats)
        
        # ç”Ÿæˆä¸»å¤„ç†æŠ¥å‘Š
        duration = time.time() - start_time
        main_report = generate_report(manager, duration)
        logger.info(main_report)
        
        # å¼€å§‹é‡è¯•å¤„ç†
        logger.info("\n[ä¸»ç¨‹åº] ğŸ”„ å¼€å§‹é‡è¯•å¤„ç†å¤±è´¥è®°å½•")
        manager = SeaTableManager(config, base_token)
        manager.image_history = image_history  # ä½¿ç”¨å…¨å±€çš„å†å²è®°å½•ç®¡ç†å™¨
        manager.retry_failed_images()
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼ˆåŒ…å«é‡è¯•ç»“æœï¼‰
        final_duration = time.time() - start_time
        final_report = generate_report(manager, final_duration)
        logger.info(final_report)
        notify_status('SeaTableå›¾ç‰‡åŒæ­¥', final_report)
        
        # è¾“å‡ºå¤±è´¥è¯¦æƒ…
        logger.info("\n" + "=" * 50)
        logger.info("å¤±è´¥è®°å½•è¯¦æƒ…")
        logger.info("=" * 50)
        
        failed_details = manager.image_history.get_failed_records()
        if not failed_details:
            logger.info("æ²¡æœ‰å¤±è´¥è®°å½•")
        else:
            for record in failed_details:
                logger.info(f"\nBase: {record['base_name']}")
                logger.info(f"Table: {record['table_name']}")
                logger.info(f"æ•°æ®: {record['row_data']}")
                logger.info(f"é“¾æ¥: {record['url']}")
                logger.info(f"é”™è¯¯: {record['error']}")
        
        logger.info("=" * 50)
        
        # æ¸…ç†æ‰€æœ‰è®°å½•ï¼ˆæ”¾åœ¨æœ€åï¼‰
        manager.image_history.clear_all_records()
        
        logger.info("[ä¸»ç¨‹åº] âœ¨ æ‰€æœ‰å¤„ç†å®Œæˆ")
        
    except Exception as e:
        logger.error(f"[ä¸»ç¨‹åº] âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
        notify_status('SeaTableå›¾ç‰‡åŒæ­¥å¼‚å¸¸', str(e))
        raise
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        cleanup_temp_files()

if __name__ == '__main__':
    main()