from seatable_api import Base
import requests
import re
import time
import json
from datetime import datetime
import sys
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
import random
import hashlib

# é…ç½®é‡è¯•ç­–ç•¥
retry_strategy = Retry(
    total=5,
    backoff_factor=1,
    status_forcelist=[429, 500, 502, 503, 504],
    allowed_methods=["HEAD", "GET", "OPTIONS", "POST"],
    connect=5,
    read=5,
    redirect=3
)

# åˆ›å»ºå¸¦é‡è¯•çš„session
session = requests.Session()
adapter = HTTPAdapter(
    max_retries=retry_strategy,
    pool_connections=10,
    pool_maxsize=10,
    pool_block=False
)
session.mount("http://", adapter)
session.mount("https://", adapter)

class RunningHistory:
    def __init__(self):
        self.history = {}
        self.stats = {
            'reused': 0,  # é‡å¤ä½¿ç”¨çš„æ¬¡æ•°
            'new': 0      # æ–°æå–çš„æ¬¡æ•°
        }
    
    def get_url_hash(self, url):
        """è·å–URLçš„å“ˆå¸Œå€¼"""
        # ç§»é™¤URLä¸­çš„è·Ÿè¸ªå‚æ•°ï¼Œåªä¿ç•™åŸºæœ¬çš„äº§å“IDéƒ¨åˆ†
        clean_url = re.sub(r'/ref=.*$', '', url)
        clean_url = re.sub(r'\?.*$', '', clean_url)
        return hashlib.md5(clean_url.encode()).hexdigest()
    
    def add_record(self, url, image_url, success=True):
        """æ·»åŠ è®°å½•"""
        url_hash = self.get_url_hash(url)
        self.history[url_hash] = {
            'url': url,
            'image_url': image_url,
            'success': success,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        self.stats['new'] += 1
    
    def get_record(self, url):
        """è·å–è®°å½•"""
        url_hash = self.get_url_hash(url)
        record = self.history.get(url_hash)
        if record:
            self.stats['reused'] += 1
        return record

def print_log(message, status=""):
    """
    é’é¾™é¢æ¿æ ‡å‡†æ—¥å¿—è¾“å‡ºæ ¼å¼
    status: å¯ä»¥æ˜¯ [SUCCESS] [ERROR] [WARN] [INFO] ç­‰
    """
    time_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    if status:
        print(f"{time_str} [{status}] {message}")
    else:
        print(f"{time_str} {message}")

def get_random_delay():
    """
    ç”Ÿæˆéšæœºå»¶è¿Ÿæ—¶é—´ï¼ˆ1-3ç§’ï¼‰
    """
    return random.uniform(1, 3)

def create_seatable_connection(api_token, server_url, max_retries=3):
    """
    åˆ›å»ºSeaTableè¿æ¥ï¼Œå¸¦é‡è¯•æœºåˆ¶
    """
    for attempt in range(max_retries):
        try:
            base = Base(api_token, server_url)
            base.auth()
            return base
        except Exception as e:
            if attempt < max_retries - 1:
                delay = random.uniform(1, 3)
                print_log(f"SeaTableè¿æ¥å¤±è´¥ï¼Œ{delay:.1f}ç§’åé‡è¯•: {str(e)}", "WARN")
                time.sleep(delay)
            else:
                raise e

def update_row_with_retry(base, table_name, row_id, data, max_retries=3):
    """
    æ›´æ–°è¡Œæ•°æ®ï¼Œå¸¦é‡è¯•æœºåˆ¶
    """
    for attempt in range(max_retries):
        try:
            base.update_row(table_name, row_id, data)
            return True
        except Exception as e:
            if attempt < max_retries - 1:
                delay = random.uniform(1, 3)
                print_log(f"æ›´æ–°æ•°æ®å¤±è´¥ï¼Œ{delay:.1f}ç§’åé‡è¯•: {str(e)}", "WARN")
                time.sleep(delay)
            else:
                raise e

def get_amazon_domain(url):
    """
    è·å–äºšé©¬é€Šé“¾æ¥çš„åŸŸåå’Œç«™ç‚¹ä¿¡æ¯
    """
    domain_map = {
        'amazon.com': 'US',
        'amazon.ca': 'CA',
        'amazon.co.uk': 'UK',
        'amazon.de': 'DE',
        'amazon.fr': 'FR',
        'amazon.it': 'IT',
        'amazon.es': 'ES',
        'amazon.co.jp': 'JP',
        'amazon.com.au': 'AU',
        'amazon.in': 'IN',
        'amazon.com.mx': 'MX',
        'amazon.com.br': 'BR',
        'amazon.nl': 'NL',
        'amazon.sg': 'SG',
        'amazon.ae': 'AE',
        'amazon.sa': 'SA',
        'amazon.se': 'SE',
        'amazon.pl': 'PL',
        'amazon.tr': 'TR'
    }
    
    for domain in domain_map:
        if domain in url:
            return domain, domain_map[domain]
    return 'amazon.com', 'US'  # é»˜è®¤è¿”å›ç¾å›½ç«™ç‚¹

def get_amazon_image(url, history, max_retries=5):
    """
    ä»äºšé©¬é€Šäº§å“é¡µé¢è·å–ä¸»å›¾ç‰‡URLï¼Œå¸¦é‡è¯•æœºåˆ¶
    """
    print_log(f"å¼€å§‹å¤„ç†é“¾æ¥: {url}", "INFO")
    
    # æ£€æŸ¥è¿è¡Œä¸­çš„å†å²è®°å½•
    history_record = history.get_record(url)
    if history_record:
        if history_record['success']:
            print_log(f"ä»å†å²è®°å½•ä¸­è·å–å›¾ç‰‡: {history_record['image_url']}", "SUCCESS")
            return history_record['image_url']
        else:
            print_log("è¯¥é“¾æ¥ä¹‹å‰è·å–å¤±è´¥ï¼Œå°†é‡æ–°å°è¯•", "WARN")
    
    # å¤„ç†URLæ ¼å¼
    if not url.startswith('http'):
        url = 'https://' + url
    
    # ç§»é™¤URLä¸­çš„è·Ÿè¸ªå‚æ•°ï¼Œåªä¿ç•™åŸºæœ¬çš„äº§å“ID
    url = re.sub(r'/ref=.*$', '', url)  # ç§»é™¤refå‚æ•°
    url = re.sub(r'\?.*$', '', url)     # ç§»é™¤æ‰€æœ‰æŸ¥è¯¢å‚æ•°
    
    # è·å–ç«™ç‚¹ä¿¡æ¯
    domain, site = get_amazon_domain(url)
    print_log(f"è¯†åˆ«åˆ°ç«™ç‚¹: {site} ({domain})", "INFO")
    
    # æ›´å®Œæ•´çš„è¯·æ±‚å¤´
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0'
    }
    
    for attempt in range(max_retries):
        try:
            if attempt == 0:
                initial_delay = random.uniform(2, 4)
                print_log(f"åˆå§‹ç­‰å¾… {initial_delay:.1f} ç§’...", "INFO")
                time.sleep(initial_delay)
            elif attempt > 0:
                delay = random.uniform(3, 6)
                print_log(f"ç¬¬ {attempt + 1} æ¬¡é‡è¯•è·å–å›¾ç‰‡... (ç­‰å¾… {delay:.1f} ç§’)", "WARN")
                time.sleep(delay)
            
            # é…ç½®è¯·æ±‚é€‰é¡¹
            request_options = {
                'headers': headers,
                'timeout': (10, 20),  # (è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶)
                'allow_redirects': True,
                'verify': True,
                'stream': True
            }
            
            # è·å–é¡µé¢å†…å®¹
            with session.get(url, **request_options) as response:
                if response.status_code == 301 or response.status_code == 302:
                    url = response.headers['Location']
                    print_log(f"è·Ÿéšé‡å®šå‘åˆ°: {url}", "INFO")
                    continue
                    
                if response.status_code != 200:
                    print_log(f"é¡µé¢è¯·æ±‚å¤±è´¥: HTTP {response.status_code}", "ERROR")
                    continue
                
                # ä½¿ç”¨æµå¼è¯»å–å†…å®¹
                content = ''
                for chunk in response.iter_content(chunk_size=8192, decode_unicode=True):
                    if chunk:
                        content += chunk
                
                # 1. é¦–å…ˆå°è¯•ä»data-a-dynamic-imageä¸­è·å–ï¼ˆé€šå¸¸åŒ…å«æœ€é«˜è´¨é‡çš„å›¾ç‰‡ï¼‰
                dynamic_image_match = re.search(r'data-a-dynamic-image="([^"]+)"', content)
                if dynamic_image_match:
                    try:
                        image_dict = json.loads(dynamic_image_match.group(1).replace('&quot;', '"'))
                        # è·å–æœ€å¤§å°ºå¯¸çš„å›¾ç‰‡URL
                        largest_image = max(image_dict.items(), key=lambda x: int(x[1][0]) * int(x[1][1]))
                        image_url = largest_image[0]
                        print_log(f"æˆåŠŸè·å–åŠ¨æ€å›¾ç‰‡: {image_url}", "SUCCESS")
                        history.add_record(url, image_url, True)
                        return image_url
                    except:
                        pass
                
                # 2. å°è¯•å…¶ä»–å›¾ç‰‡URLæ¨¡å¼
                image_patterns = [
                    # é«˜ï¿½ï¿½ï¿½å›¾ç‰‡URL
                    r'data-old-hires="(https://[^"]+)"',
                    r'data-zoom-hires="(https://[^"]+)"',
                    r'data-a-dynamic-image="([^"]+)"',
                    # ä¸»å›¾ç‰‡URL
                    r'id="landingImage"[^>]+src="(https://[^"]+)"',
                    r'id="imgBlkFront"[^>]+src="(https://[^"]+)"',
                    r'id="main-image-container"[^>]+href="(https://[^"]+)"',
                    # äº§å“å›¾ç‰‡URL
                    r'"large":"(https://[^"]+\.jpg)"',
                    r'"main":"(https://[^"]+\.jpg)"',
                    r'"mainUrl":"(https://[^"]+)"',
                    # å¤‡ç”¨å›¾ç‰‡URL
                    r'data-a-image-name="[^"]*"[^>]*src="(https://[^"]+)"',
                    r'id="main-image"[^>]+src="(https://[^"]+)"',
                    r'class="a-dynamic-image"[^>]+src="(https://[^"]+)"'
                ]
                
                for pattern in image_patterns:
                    matches = re.findall(pattern, content)
                    if matches:
                        for match in matches:
                            if isinstance(match, tuple):
                                match = match[0]
                            image_url = match.replace('\\', '')
                            
                            # ç¡®ä¿è·å–é«˜æ¸…å›¾ç‰‡
                            if '_SL1500_' not in image_url and '._AC_' in image_url:
                                image_url = image_url.replace('._AC_', '._AC_SL1500_')
                            elif '_SX' in image_url:
                                image_url = re.sub(r'\._SX\d+_', '._SX1500_', image_url)
                            elif '_SY' in image_url:
                                image_url = re.sub(r'\._SY\d+_', '._SY1500_', image_url)
                            elif '_SR' in image_url:
                                image_url = re.sub(r'\._SR\d+,\d+_', '._SR1500,1500_', image_url)
                            
                            # éªŒè¯å›¾ç‰‡URLæ˜¯å¦æœ‰æ•ˆ
                            if 'sprite' not in image_url.lower() and 'placeholder' not in image_url.lower():
                                print_log(f"æˆåŠŸè·å–å›¾ç‰‡: {image_url}", "SUCCESS")
                                history.add_record(url, image_url, True)
                                return image_url
                
                print_log(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•æœªæ‰¾åˆ°å›¾ç‰‡", "WARN")
                
        except requests.Timeout:
            print_log(f"ç¬¬ {attempt + 1} æ¬¡è¯·æ±‚è¶…æ—¶", "ERROR")
        except requests.TooManyRedirects:
            print_log(f"ç¬¬ {attempt + 1} æ¬¡è¯·æ±‚é‡å®šå‘æ¬¡æ•°è¿‡å¤š", "ERROR")
            # å¦‚æœé‡å®šå‘æ¬¡æ•°è¿‡å¤šï¼Œå°è¯•æ¸…é™¤URLä¸­çš„å‚æ•°
            url = re.sub(r'\?.*$', '', url)
        except Exception as e:
            print_log(f"ç¬¬ {attempt + 1} æ¬¡è¯·æ±‚å‡ºé”™: {str(e)}", "ERROR")
    
    print_log(f"ç»è¿‡ {max_retries} æ¬¡å°è¯•åä»æœªæ‰¾åˆ°å›¾ç‰‡", "ERROR")
    history.add_record(url, None, False)
    return None

def process_single_row(row, base, table_name, total, index, history):
    """
    å¤„ç†å•è¡Œæ•°æ®
    """
    try:
        print_log(f"å¼€å§‹å¤„ç†ç¬¬ {index}/{total} è¡Œæ•°æ®:", "INFO")
        product_url = row.get('äº§å“é“¾æ¥')
        
        if not product_url:
            print_log("äº§å“é“¾æ¥ä¸ºç©ºï¼Œè·³è¿‡å¤„ç†", "WARN")
            return {'status': 'skipped', 'reason': 'empty_link'}
        
        # è·å–å½“å‰çš„å›¾ç‰‡URL
        current_image = row.get('äº§å“å›¾ç‰‡')
        current_image_url = None
        if current_image and isinstance(current_image, list) and len(current_image) > 0:
            current_image_url = current_image[0]
            print_log(f"å½“å‰å·²æœ‰å›¾ç‰‡: {current_image_url}", "INFO")
            
        # è·å–æ–°çš„å›¾ç‰‡URLï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        new_image_url = get_amazon_image(product_url, history)
        if not new_image_url:
            print_log("è·å–æ–°å›¾ç‰‡å¤±è´¥ï¼Œä¿æŒåŸå›¾ç‰‡ä¸å˜", "WARN")
            if current_image_url:
                print_log("ä¿ç•™åŸæœ‰å›¾ç‰‡", "INFO")
                return {'status': 'skipped', 'reason': 'kept_original'}
            else:
                print_log("æ— åŸæœ‰å›¾ç‰‡", "WARN")
                return {'status': 'failed', 'reason': 'no_image'}
                
        # æ¯”è¾ƒæ–°æ—§å›¾ç‰‡URL
        if current_image_url == new_image_url:
            print_log("å›¾ç‰‡æœªå‘ç”Ÿå˜åŒ–ï¼Œæ— éœ€æ›´æ–°", "INFO")
            return {'status': 'skipped', 'reason': 'unchanged'}
            
        # æ›´æ–°äº§å“å›¾ç‰‡åˆ—
        row_id = row['_id']
        print_log(f"æ­£åœ¨æ›´æ–°å›¾ç‰‡...", "INFO")
        try:
            if update_row_with_retry(base, table_name, row_id, {'äº§å“å›¾ç‰‡': [new_image_url]}):
                print_log(f"å›¾ç‰‡æ›´æ–°æˆåŠŸ", "SUCCESS")
                return {'status': 'updated'}
        except Exception as e:
            print_log(f"å›¾ç‰‡æ›´æ–°å¤±è´¥: {str(e)}", "ERROR")
            if current_image_url:
                print_log("ä¿ç•™åŸæœ‰å›¾ç‰‡", "INFO")
                return {'status': 'skipped', 'reason': 'kept_original'}
            else:
                return {'status': 'failed', 'reason': 'update_failed'}
                
    except Exception as e:
        print_log(f"å¤„ç†æ•°æ®æ—¶å‡ºé”™: {str(e)}", "ERROR")
        return {'status': 'failed', 'reason': 'process_error'}

def main():
    """
    ä¸»å‡½æ•°
    """
    start_time = datetime.now()
    print_log("="*50)
    print_log("å¼€å§‹æ‰§è¡Œè‡ªåŠ¨è·å–äºšé©¬é€Šäº§å“å›¾ç‰‡ä»»åŠ¡", "INFO")
    print_log("="*50)
    
    server_url = 'https://cloud.seatable.cn'
    api_token = '7d67fb2e9a309d5f5c25099d65c844a01d7c6c40'
    
    try:
        print_log("æ­£åœ¨è¿æ¥åˆ°SeaTable...", "INFO")
        base = create_seatable_connection(api_token, server_url)
        print_log("SeaTableè¿æ¥æˆåŠŸ", "SUCCESS")
        
        # è·å–è¡¨æ ¼æ•°æ®
        table_name = "Task"
        print_log(f"æ­£åœ¨è·å–è¡¨æ ¼ {table_name} çš„æ•°æ®...", "INFO")
        rows = base.list_rows(table_name)
        total_rows = len(rows)
        print_log(f"æˆåŠŸè·å– {total_rows} è¡Œæ•°æ®", "SUCCESS")
        
        # åˆå§‹åŒ–ç»“æœç»Ÿè®¡
        results = {
            'updated': 0,
            'skipped': 0,
            'failed': 0,
            'empty_links': 0,
            'unchanged': 0,
            'kept_original': 0,
            'from_history': 0
        }
        
        # åˆ›å»ºè¿è¡Œæ—¶å†å²è®°å½•å¯¹è±¡
        history = RunningHistory()
        
        # é¡ºåºå¤„ç†æ¯ä¸€è¡Œæ•°æ®
        for index, row in enumerate(rows, 1):
            print_log("-"*30)
            
            # å¤„ç†å•è¡Œæ•°æ®
            result = process_single_row(row, base, table_name, total_rows, index, history)
            
            # æ›´æ–°ç»Ÿè®¡
            if result['status'] == 'updated':
                results['updated'] += 1
            elif result['status'] == 'skipped':
                results['skipped'] += 1
                if result['reason'] == 'empty_link':
                    results['empty_links'] += 1
                elif result['reason'] == 'unchanged':
                    results['unchanged'] += 1
                elif result['reason'] == 'kept_original':
                    results['kept_original'] += 1
            elif result['status'] == 'failed':
                results['failed'] += 1
            
            # æ˜¾ç¤ºè¿›åº¦
            progress = index / total_rows * 100
            print_log(f"å½“å‰è¿›åº¦: {progress:.1f}% ({index}/{total_rows})", "INFO")
            
            # å¤„ç†é—´éš”
            if index < total_rows:
                delay = random.uniform(2, 4)
                print_log(f"ç­‰å¾… {delay:.1f} ç§’åç»§ç»­...", "INFO")
                time.sleep(delay)
        
        # è®¡ç®—æ€»è€—æ—¶
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        print_log("\n" + "="*50)
        print_log("ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼Œæœ€ç»ˆç»Ÿè®¡ï¼š", "INFO")
        print_log("="*50)
        print_log(f"æ€»å¤„ç†æ•°: {total_rows} æ¡æ•°æ®", "INFO")
        print_log(f"æ‰§è¡Œè€—æ—¶: {duration:.1f} ç§’", "INFO")
        print_log("-"*50)
        print_log("å¤„ç†ç»“æœè¯¦æƒ…:", "INFO")
        print_log(f"âœ… æ›´æ–°æˆåŠŸ: {results['updated']} æ¡", "SUCCESS")
        print_log(f"â­ï¸ æ— éœ€æ›´æ–°: {results['unchanged']} æ¡ (å›¾ç‰‡æœªå˜åŒ–)", "INFO")
        print_log(f"âš ï¸ ç©ºé“¾æ¥æ•°: {results['empty_links']} æ¡", "WARN")
        print_log(f"ğŸ“ ä¿ç•™åŸå›¾: {results['kept_original']} æ¡", "INFO")
        print_log(f"âŒ å®Œå…¨å¤±è´¥: {results['failed']} æ¡ (æ— æ³•è·å–æ–°å›¾ç‰‡ä¸”æ— åŸå›¾)", "ERROR")
        print_log(f"â™»ï¸ é‡å¤é“¾æ¥: {history.stats['reused']} æ¡", "INFO")
        print_log("-"*50)
        print_log("æ±‡æ€»ç»Ÿè®¡:", "INFO")
        print_log(f"âœ… æˆåŠŸå¤„ç†: {results['updated']} æ¡", "SUCCESS")
        print_log(f"â­ï¸ è·³è¿‡å¤„ç†: {results['skipped']} æ¡", "INFO")
        print_log(f"âŒ å¤„ç†å¤±è´¥: {results['failed']} æ¡", "ERROR")
        print_log("="*50)
        
    except Exception as e:
        print_log(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}", "ERROR")
        
if __name__ == '__main__':
    main() 